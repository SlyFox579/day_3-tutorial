{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SlyFox579/day_3-tutorial/blob/main/simple_pandas_pyspark_agg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqVE-my11j7K"
      },
      "source": [
        "# Exploring Spark with Pandas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWhVKGrH1j7R"
      },
      "source": [
        "Using pandas examples, convert the analysis to pyspark. This is useful if you discover your data grows too large for your tooling.\n",
        "\n",
        "The purpose of this notebook is to familiarise yourself you the pyspark API. You are welcome to use the R version of this if you wish. As long as you are able to obtain the correct results. We will be using python in this notebook as it is quite widely used through data science and the community is very large.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b3V9Qgr1j7T"
      },
      "source": [
        "#### Firstly, let's get our spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnvUOUpv1j7U"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd \n",
        "spark = SparkSession.builder.appName('panda-and-spark').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01GTFWku1j7g"
      },
      "outputs": [],
      "source": [
        "# save to the filesystem to prevent another load\n",
        "! curl -o rows.csv https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ93jNbn1j7h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "nyc_df = pd.read_csv('rows.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjtfI4q01j7h"
      },
      "outputs": [],
      "source": [
        "# number or rows\n",
        "\n",
        "print(len(nyc_df))\n",
        "\n",
        "# this is quite large so we will work with a sample while we experiment in pandas as least."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msg2rLMW1j7i"
      },
      "source": [
        "We'll take a random sample at 20% of the original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xkQ4htp1j7i"
      },
      "outputs": [],
      "source": [
        "nyc_small = nyc_df.sample(frac=0.2, replace=False, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFn_78np1j7j"
      },
      "outputs": [],
      "source": [
        "# we are also going to limit the columns to those we are going to work with\n",
        "\n",
        "nyc_small = nyc_small[['CRASH DATE', 'CONTRIBUTING FACTOR VEHICLE 1', \n",
        "                       'BOROUGH', 'VEHICLE TYPE CODE 1', \n",
        "                       'NUMBER OF PERSONS INJURED']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GxSgkrP1j7j"
      },
      "outputs": [],
      "source": [
        "nyc_small.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNiDI7hI1j7k"
      },
      "source": [
        "Now, let's create the pyspark dataframe. Now we two frames with the same content\n",
        "  * nyc_small: pandas\n",
        "  * sdf_small: pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWolG-qa1j7l"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SQLContext\n",
        "\n",
        "\n",
        "# there are nan's in the frame with strings, and spark can't 'infer the schema', so we have to help it out \n",
        "# by replacing them with empty strings and forcing the column to be a string\n",
        "\n",
        "sdf_small = SQLContext(spark).createDataFrame(nyc_small.fillna('').astype('str'))\n",
        "\n",
        "\n",
        "# Lets check the schema quickly\n",
        "\n",
        "print(sdf_small.schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQHdyYyY1j7m"
      },
      "source": [
        "# Questions\n",
        "\n",
        "Answer the following questions by porting the pandas code to the Spark API\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKIs_x2F1j7m"
      },
      "source": [
        "\n",
        "# Question 1\n",
        "\n",
        "\n",
        "> On what day do most crashes occcur?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZKXLqdA1j7n"
      },
      "outputs": [],
      "source": [
        "# Pandas\n",
        "nyc_small.groupby('CRASH DATE')['CRASH DATE'].count().sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sagu7HHj1j7n"
      },
      "outputs": [],
      "source": [
        "### Spark?\n",
        "\n",
        "sdf_small.groupBy('CRASH DATE').count().orderBy('count', ascending=False).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me8adRnl1j7o"
      },
      "source": [
        "# Question 2\n",
        "\n",
        "> _Where do most crashes occur?_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdc4hdpn1j7o"
      },
      "outputs": [],
      "source": [
        "nyc_small.groupby('BOROUGH')['BOROUGH'].count().sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_8Vrn981j7o"
      },
      "outputs": [],
      "source": [
        "## Spark?\n",
        "\n",
        "sdf_small.groupBy('BOROUGH').count().orderBy('count', ascending=False).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwhpQztC1j7o"
      },
      "source": [
        " # Question 3\n",
        " \n",
        " > What is the most common cause of accident in 'QUEENS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LWHXjV41j7p"
      },
      "outputs": [],
      "source": [
        "nyc_small[(nyc_small.BOROUGH == 'QUEENS')]['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()\n",
        "\n",
        "# you can also use a group by (to avoid the pandas value_counts function)\n",
        "\n",
        "nyc_small[(nyc_small.BOROUGH == 'QUEENS')].groupby(\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1'\n",
        ")['CONTRIBUTING FACTOR VEHICLE 1'].count().sort_values(ascending=False).head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lze8kYj_1j7q"
      },
      "outputs": [],
      "source": [
        "## Spark?\n",
        "\n",
        "df = sdf_small.filter(sdf_small.BOROUGH  == \"QUEENS\").groupBy('CONTRIBUTING FACTOR VEHICLE 1')\\\n",
        ".agg(f.count(f.lit(1)).alias('total_freq'))\\\n",
        ".orderBy('total_freq', ascending=False)\n",
        "\n",
        "df.show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNeLX9TJ1j7q"
      },
      "source": [
        "# Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkK5T-EB1j7r"
      },
      "source": [
        "> _What is the average number or injuries for specific cars driving in specific suburbs_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKtMR4Yn1j7r"
      },
      "outputs": [],
      "source": [
        "nyc_small.groupby(['VEHICLE TYPE CODE 1', 'BOROUGH'])['NUMBER OF PERSONS INJURED'].mean().sort_values(ascending=False).head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSiiog8h1j7s"
      },
      "outputs": [],
      "source": [
        "## Spark?\n",
        "\n",
        "df = sdf_small.groupBy('VEHICLE TYPE CODE 1', 'BOROUGH')\\\n",
        ".agg(f.avg('NUMBER OF PERSONS INJURED').alias('avg_injuries'))\\\n",
        ".orderBy('avg_injuries', ascending=False)\n",
        "\n",
        "df.show(3)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "simple-pandas-pyspark-agg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}